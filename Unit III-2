import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import matplotlib.pyplot as plt
from typing import Tuple, List

class IntersectionEnv:
    """
    Environment for 4-way intersection with traffic lights
    """
    def __init__(self):
        # State space: [car_x, car_y, velocity_x, velocity_y, 
        #              traffic_light_state, timestep_in_light_cycle,
        #              other_car_1_x, other_car_1_y, ...]
        self.grid_size = 20
        self.intersection_center = (self.grid_size // 2, self.grid_size // 2)
        self.intersection_size = 6
        
        # Traffic light states: 0=NS green, 1=EW green
        self.light_cycle = 20  # seconds per light phase
        self.light_state = 0
        self.time_in_cycle = 0
        
        # Car parameters
        self.car_pos = [5, self.grid_size // 2]  # Starting from west
        self.car_vel = [0, 0]
        self.target_pos = [self.grid_size - 5, self.grid_size // 2]  # Goal: east side
        
        # Other cars for collision detection
        self.other_cars = self._generate_other_cars()
        
        # Action space: 0=accelerate forward, 1=decelerate, 2=turn left, 3=turn right, 4=stop
        self.action_space = 5
        self.state_space = 4 + 2 + 4 * 3  # car pos/vel + light state + 3 other cars
        
        # Physics
        self.max_speed = 2.0
        self.acceleration = 0.5
        self.deceleration = 0.7
        
        # Collision parameters
        self.collision_distance = 1.5
        
    def _generate_other_cars(self):
        """Generate other cars in the intersection"""
        other_cars = []
        # Add cars from different directions
        other_cars.append([self.grid_size // 2, 15])  # From north
        other_cars.append([15, self.grid_size // 2])  # From east
        other_cars.append([self.grid_size // 2, 5])   # From south
        return other_cars
    
    def reset(self):
        """Reset environment to initial state"""
        self.car_pos = [5, self.grid_size // 2]
        self.car_vel = [0, 0]
        self.light_state = 0
        self.time_in_cycle = 0
        self.other_cars = self._generate_other_cars()
        return self._get_state()
    
    def _get_state(self):
        """Get current state as numpy array"""
        state = [
            self.car_pos[0] / self.grid_size,  # Normalized x
            self.car_pos[1] / self.grid_size,  # Normalized y
            self.car_vel[0] / self.max_speed,  # Normalized velocity x
            self.car_vel[1] / self.max_speed,  # Normalized velocity y
            float(self.light_state),  # Traffic light state
            self.time_in_cycle / self.light_cycle,  # Normalized time in cycle
        ]
        
        # Add other cars' positions
        for car in self.other_cars:
            state.extend([
                car[0] / self.grid_size,
                car[1] / self.grid_size
            ])
        
        return np.array(state, dtype=np.float32)
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, dict]:
        """
        Take a step in the environment
        
        Args:
            action: Action to take
            
        Returns:
            next_state, reward, done, info
        """
        reward = -1  # Time penalty
        
        # Update traffic light
        self.time_in_cycle += 1
        if self.time_in_cycle >= self.light_cycle:
            self.time_in_cycle = 0
            self.light_state = 1 - self.light_state  # Toggle light
        
        # Apply action
        self._apply_action(action)
        
        # Update car position
        self.car_pos[0] += self.car_vel[0]
        self.car_pos[1] += self.car_vel[1]
        
        # Update other cars (simplified movement)
        self._update_other_cars()
        
        # Check collisions
        collision = self._check_collision()
        if collision:
            reward -= 10  # Collision penalty
            done = True
        else:
            done = False
            
        # Check if reached destination
        destination_reached = (abs(self.car_pos[0] - self.target_pos[0]) < 2 and 
                             abs(self.car_pos[1] - self.target_pos[1]) < 2)
        
        if destination_reached:
            reward += 20  # Arrival bonus
            done = True
            
        # Boundary check
        if (self.car_pos[0] < 0 or self.car_pos[0] >= self.grid_size or
            self.car_pos[1] < 0 or self.car_pos[1] >= self.grid_size):
            reward -= 5  # Boundary penalty
            done = True
            
        return self._get_state(), reward, done, {
            'collision': collision,
            'destination_reached': destination_reached
        }
    
    def _apply_action(self, action: int):
        """Apply the selected action to control the car"""
        if action == 0:  # Accelerate forward
            if abs(self.car_vel[0]) + abs(self.car_vel[1]) < self.max_speed:
                if abs(self.car_vel[0]) > abs(self.car_vel[1]):
                    self.car_vel[0] += self.acceleration * (1 if self.car_vel[0] >= 0 else -1)
                else:
                    self.car_vel[1] += self.acceleration * (1 if self.car_vel[1] >= 0 else -1)
                    
        elif action == 1:  # Decelerate
            if self.car_vel[0] != 0:
                self.car_vel[0] -= self.deceleration * (1 if self.car_vel[0] > 0 else -1)
                if abs(self.car_vel[0]) < 0.1:
                    self.car_vel[0] = 0
            if self.car_vel[1] != 0:
                self.car_vel[1] -= self.deceleration * (1 if self.car_vel[1] > 0 else -1)
                if abs(self.car_vel[1]) < 0.1:
                    self.car_vel[1] = 0
                    
        elif action == 2:  # Turn left (assuming moving east initially)
            temp = self.car_vel[0]
            self.car_vel[0] = -self.car_vel[1] * 0.8  # Reduce speed during turn
            self.car_vel[1] = temp * 0.8
            
        elif action == 3:  # Turn right
            temp = self.car_vel[0]
            self.car_vel[0] = self.car_vel[1] * 0.8
            self.car_vel[1] = -temp * 0.8
            
        elif action == 4:  # Stop
            self.car_vel[0] = 0
            self.car_vel[1] = 0
    
    def _update_other_cars(self):
        """Update positions of other cars in intersection"""
        for i, car in enumerate(self.other_cars):
            # Simple movement patterns
            if i == 0:  # North car moves south
                car[1] -= 1 if self.light_state == 0 else 0  # Only move on NS green
            elif i == 1:  # East car moves west
                car[0] -= 1 if self.light_state == 1 else 0  # Only move on EW green
            elif i == 2:  # South car moves north
                car[1] += 1 if self.light_state == 0 else 0
    
    def _check_collision(self) -> bool:
        """Check for collisions with other cars or intersection boundaries"""
        # Check collision with other cars
        for car in self.other_cars:
            distance = np.sqrt((self.car_pos[0] - car[0])**2 + 
                             (self.car_pos[1] - car[1])**2)
            if distance < self.collision_distance:
                return True
                
        # Check if in intersection during red light
        in_intersection = (
            abs(self.car_pos[0] - self.intersection_center[0]) < self.intersection_size // 2 and
            abs(self.car_pos[1] - self.intersection_center[1]) < self.intersection_size // 2
        )
        
        # If car is going east-west (x velocity dominant)
        if abs(self.car_vel[0]) > abs(self.car_vel[1]):
            if in_intersection and self.light_state == 0:  # EW has red light
                return True
        else:  # Going north-south
            if in_intersection and self.light_state == 1:  # NS has red light
                return True
                
        return False

class ActorNetwork(nn.Module):
    """Actor network for policy"""
    def __init__(self, state_dim, action_dim):
        super(ActorNetwork, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
            nn.Softmax(dim=-1)
        )
    
    def forward(self, state):
        return self.network(state)

class CriticNetwork(nn.Module):
    """Critic network for value function"""
    def __init__(self, state_dim):
        super(CriticNetwork, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
    
    def forward(self, state):
        return self.network(state)

class A2CAgent:
    """Advantage Actor-Critic Agent"""
    def __init__(self, state_dim, action_dim, lr_actor=0.001, lr_critic=0.001, gamma=0.99):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        
        # Networks
        self.actor = ActorNetwork(state_dim, action_dim)
        self.critic = CriticNetwork(state_dim)
        
        # Optimizers
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)
        
        # Storage for trajectories
        self.log_probs = []
        self.values = []
        self.rewards = []
        self.dones = []
        
    def select_action(self, state):
        """Select action based on current policy"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        
        # Get action probabilities
        action_probs = self.actor(state_tensor)
        
        # Create categorical distribution and sample action
        dist = torch.distributions.Categorical(action_probs)
        action = dist.sample()
        
        # Get value estimate
        value = self.critic(state_tensor)
        
        # Store for training
        self.log_probs.append(dist.log_prob(action))
        self.values.append(value)
        
        return action.item()
    
    def update(self):
        """Update networks using stored trajectories"""
        # Calculate returns and advantages
        returns = []
        advantages = []
        
        R = 0
        for reward, done, value in zip(reversed(self.rewards), 
                                      reversed(self.dones), 
                                      reversed(self.values)):
            R = reward + self.gamma * R * (1 - done)
            returns.insert(0, R)
            
        # Convert to tensors
        returns = torch.tensor(returns)
        values = torch.cat(self.values)
        
        # Calculate advantages
        advantages = returns - values.squeeze()
        
        # Normalize advantages
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # Actor loss (policy gradient)
        actor_loss = -torch.stack(self.log_probs) * advantages.detach()
        actor_loss = actor_loss.mean()
        
        # Critic loss (value function error)
        critic_loss = nn.MSELoss()(values.squeeze(), returns)
        
        # Update networks
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()
        
        # Clear storage
        self.log_probs = []
        self.values = []
        self.rewards = []
        self.dones = []
        
        return actor_loss.item(), critic_loss.item()
    
    def store_transition(self, reward, done):
        """Store reward and done flag for current step"""
        self.rewards.append(reward)
        self.dones.append(done)

def train_a2c(episodes=1000, max_steps=100):
    """Train the A2C agent"""
    env = IntersectionEnv()
    agent = A2CAgent(env.state_space, env.action_space)
    
    episode_rewards = []
    episode_collisions = []
    episode_successes = []
    
    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        collision = False
        success = False
        
        for step in range(max_steps):
            # Select action
            action = agent.select_action(state)
            
            # Take step in environment
            next_state, reward, done, info = env.step(action)
            
            # Store transition
            agent.store_transition(reward, done)
            
            # Update statistics
            total_reward += reward
            collision = collision or info['collision']
            success = success or info['destination_reached']
            
            # Move to next state
            state = next_state
            
            # Update agent if enough steps collected or episode done
            if done or step == max_steps - 1:
                if len(agent.rewards) > 0:
                    actor_loss, critic_loss = agent.update()
                break
        
        # Log episode statistics
        episode_rewards.append(total_reward)
        episode_collisions.append(1 if collision else 0)
        episode_successes.append(1 if success else 0)
        
        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(episode_rewards[-100:])
            success_rate = np.mean(episode_successes[-100:]) * 100
            collision_rate = np.mean(episode_collisions[-100:]) * 100
            print(f"Episode {episode + 1}: "
                  f"Avg Reward = {avg_reward:.2f}, "
                  f"Success Rate = {success_rate:.1f}%, "
                  f"Collision Rate = {collision_rate:.1f}%")
    
    # Plot training results
    plot_training_results(episode_rewards, episode_collisions, episode_successes, episodes)
    
    return env, agent

def plot_training_results(rewards, collisions, successes, episodes):
    """Plot training statistics"""
    fig, axes = plt.subplots(3, 1, figsize=(10, 12))
    
    # Plot rewards
    axes[0].plot(range(episodes), rewards, alpha=0.6)
    axes[0].set_xlabel('Episode')
    axes[0].set_ylabel('Total Reward')
    axes[0].set_title('Episode Rewards')
    
    # Plot moving average of rewards
    window_size = 50
    moving_avg = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')
    axes[0].plot(range(window_size-1, episodes), moving_avg, 'r-', linewidth=2)
    axes[0].legend(['Reward', f'{window_size}-episode Moving Avg'])
    
    # Plot collision rate
    axes[1].plot(range(episodes), collisions, 'r.', alpha=0.3, markersize=4)
    axes[1].set_xlabel('Episode')
    axes[1].set_ylabel('Collision (1=Yes)')
    axes[1].set_title('Collisions per Episode')
    
    # Plot success rate
    axes[2].plot(range(episodes), successes, 'g.', alpha=0.3, markersize=4)
    axes[2].set_xlabel('Episode')
    axes[2].set_ylabel('Success (1=Yes)')
    axes[2].set_title('Successes per Episode')
    
    plt.tight_layout()
    plt.savefig('a2c_training_results.png', dpi=100, bbox_inches='tight')
    plt.show()

def test_agent(env, agent, num_tests=10):
    """Test the trained agent"""
    print("\n" + "="*50)
    print("Testing Trained Agent")
    print("="*50)
    
    success_count = 0
    collision_count = 0
    total_rewards = []
    
    for test in range(num_tests):
        state = env.reset()
        total_reward = 0
        done = False
        steps = 0
        
        while not done and steps < 50:
            # Select action greedily during test
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            action_probs = agent.actor(state_tensor)
            action = torch.argmax(action_probs).item()
            
            # Take action
            next_state, reward, done, info = env.step(action)
            
            total_reward += reward
            state = next_state
            steps += 1
            
            if info['collision']:
                collision_count += 1
                break
                
            if info['destination_reached']:
                success_count += 1
                break
        
        total_rewards.append(total_reward)
        
        if test < 3:  # Print first 3 test runs
            print(f"Test {test + 1}: Steps = {steps}, "
                  f"Reward = {total_reward:.2f}, "
                  f"Collision = {info['collision']}, "
                  f"Success = {info['destination_reached']}")
    
    print(f"\nTest Results (over {num_tests} episodes):")
    print(f"Success Rate: {success_count/num_tests*100:.1f}%")
    print(f"Collision Rate: {collision_count/num_tests*100:.1f}%")
    print(f"Average Reward: {np.mean(total_rewards):.2f}")
    print(f"Average Steps: {len(total_rewards) - success_count - collision_count:.1f}")

def visualize_decision_making(env, agent):
    """Visualize agent's decision making at key states"""
    print("\n" + "="*50)
    print("Decision Making Analysis")
    print("="*50)
    
    # Test states
    test_states = [
        "Approaching intersection, green light",
        "Approaching intersection, red light",
        "In intersection with oncoming traffic",
        "Clear path to destination"
    ]
    
    # Create sample states for each scenario
    sample_states = [
        np.array([0.3, 0.5, 0.8, 0.0, 1.0, 0.1, 0.5, 0.7, 0.6, 0.5, 0.5, 0.3]),
        np.array([0.3, 0.5, 0.8, 0.0, 0.0, 0.9, 0.5, 0.7, 0.6, 0.5, 0.5, 0.3]),
        np.array([0.5, 0.5, 0.5, 0.0, 1.0, 0.5, 0.5, 0.6, 0.6, 0.5, 0.5, 0.4]),
        np.array([0.7, 0.5, 0.9, 0.0, 1.0, 0.2, 0.5, 0.8, 0.8, 0.5, 0.5, 0.2])
    ]
    
    action_names = ["Accelerate", "Decelerate", "Turn Left", "Turn Right", "Stop"]
    
    for i, (desc, state) in enumerate(zip(test_states, sample_states)):
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        
        # Get action probabilities
        with torch.no_grad():
            action_probs = agent.actor(state_tensor).numpy()[0]
            value = agent.critic(state_tensor).item()
        
        # Get recommended action
        recommended_action = np.argmax(action_probs)
        
        print(f"\nScenario {i+1}: {desc}")
        print(f"State Value Estimate: {value:.3f}")
        print("Action Probabilities:")
        for j, (name, prob) in enumerate(zip(action_names, action_probs)):
            print(f"  {name}: {prob:.3f}", end="")
            if j == recommended_action:
                print("  â† RECOMMENDED")
            else:
                print()

# Run training and testing
if __name__ == "__main__":
    # Set random seeds for reproducibility
    torch.manual_seed(42)
    np.random.seed(42)
    
    # Train the agent
    print("Starting A2C Training...")
    env, agent = train_a2c(episodes=500, max_steps=100)
    
    # Test the trained agent
    test_agent(env, agent, num_tests=20)
    
    # Analyze decision making
    visualize_decision_making(env, agent)
    
    print("\n" + "="*50)
    print("Training Complete!")
    print("="*50)
