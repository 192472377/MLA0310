import numpy as np
import random
import time
from collections import defaultdict
import matplotlib.pyplot as plt

class HouseEnvironment:
    """Simulates a house environment for the vacuum cleaner"""
    def __init__(self, width=8, height=8):
        self.width = width
        self.height = height
        self.reset()
        
        # Define house layout: 0=clean, 1=dirty, 2=furniture, 3=charging station
        self.house = np.zeros((height, width), dtype=int)
        
        # Add some furniture (obstacles)
        furniture_positions = [
            (1, 2), (1, 3), (2, 2), (2, 3),  # Table
            (5, 1), (5, 2), (6, 1), (6, 2),  # Sofa
            (3, 5), (3, 6), (4, 5), (4, 6),  # Bed
        ]
        for pos in furniture_positions:
            if 0 <= pos[0] < height and 0 <= pos[1] < width:
                self.house[pos] = 2
                
        # Add charging station
        self.charging_station = (0, 0)
        self.house[self.charging_station] = 3
        
        # Initially dirty some areas (avoid furniture)
        for _ in range(15):
            row = random.randint(0, height-1)
            col = random.randint(0, width-1)
            if self.house[row, col] == 0:  # Only make clean areas dirty
                self.house[row, col] = 1
                
        # Energy parameters
        self.max_energy = 100
        self.current_energy = self.max_energy
        self.energy_consumption = {
            'move': 1,
            'clean': 2,
            'charge': -15  # Negative means adding energy
        }
        
    def reset(self):
        """Reset the environment"""
        self.vacuum_pos = (0, 0)  # Start at charging station
        self.current_energy = self.max_energy
        self.cleaned_area = 0
        self.steps = 0
        return self.get_state()
    
    def get_state(self):
        """Get current state representation"""
        row, col = self.vacuum_pos
        # State includes: position, energy level, and whether current cell is dirty
        is_dirty = 1 if self.house[row, col] == 1 else 0
        return (row, col, self.current_energy, is_dirty)
    
    def step(self, action):
        """
        Take an action in the environment
        Actions: 0=up, 1=right, 2=down, 3=left, 4=clean, 5=charge
        """
        row, col = self.vacuum_pos
        reward = 0
        done = False
        self.steps += 1
        
        # Action execution
        if action == 0:  # Up
            new_row = max(row - 1, 0)
            new_col = col
        elif action == 1:  # Right
            new_row = row
            new_col = min(col + 1, self.width - 1)
        elif action == 2:  # Down
            new_row = min(row + 1, self.height - 1)
            new_col = col
        elif action == 3:  # Left
            new_row = row
            new_col = max(col - 1, 0)
        elif action == 4:  # Clean
            new_row, new_col = row, col
            if self.house[row, col] == 1:  # Cell is dirty
                self.house[row, col] = 0  # Clean it
                reward += 5  # Reward for cleaning
                self.cleaned_area += 1
        elif action == 5:  # Charge
            new_row, new_col = row, col
            if self.house[row, col] == 3:  # At charging station
                reward += 2  # Small reward for charging when needed
        
        # Check if new position is valid (not furniture)
        if self.house[new_row, new_col] != 2:
            self.vacuum_pos = (new_row, new_col)
        
        # Energy consumption
        if action in [0, 1, 2, 3]:  # Movement
            self.current_energy -= self.energy_consumption['move']
        elif action == 4:  # Cleaning
            self.current_energy -= self.energy_consumption['clean']
        elif action == 5:  # Charging
            if self.house[row, col] == 3:  # Only charge at station
                self.current_energy = min(self.max_energy, 
                                         self.current_energy - self.energy_consumption['charge'])
        
        # Check energy levels
        if self.current_energy <= 0:
            reward -= 10  # Penalty for running out of energy
            done = True
            
        # Check if all clean or too many steps
        if self.cleaned_area >= 15 or self.steps >= 200:
            done = True
            
        # Additional rewards/penalties
        reward -= 0.1  # Small penalty for each step (encourage efficiency)
        
        # Reward for being near dirty areas
        dirty_nearby = self.count_dirty_neighbors(row, col)
        reward += dirty_nearby * 0.5
        
        # Penalty for low energy when not near charger
        if self.current_energy < 30 and self.vacuum_pos != self.charging_station:
            reward -= 1
            
        return self.get_state(), reward, done
    
    def count_dirty_neighbors(self, row, col):
        """Count dirty cells in the 8-neighborhood"""
        count = 0
        for dr in [-1, 0, 1]:
            for dc in [-1, 0, 1]:
                if dr == 0 and dc == 0:
                    continue
                r, c = row + dr, col + dc
                if 0 <= r < self.height and 0 <= c < self.width:
                    if self.house[r, c] == 1:
                        count += 1
        return count
    
    def render(self):
        """Display the current state of the house"""
        display_house = self.house.copy()
        row, col = self.vacuum_pos
        if display_house[row, col] not in [2, 3]:  # Don't overwrite furniture or charger
            display_house[row, col] = 4  # Vacuum
        
        symbols = {
            0: 'Â·',  # Clean
            1: '*',  # Dirty
            2: 'â–ˆ',  # Furniture
            3: 'âš¡',  # Charging station
            4: 'ðŸ¤–'   # Vacuum
        }
        
        print(f"\nStep: {self.steps}, Energy: {self.current_energy}, Cleaned: {self.cleaned_area}")
        print("=" * (self.width * 2 + 3))
        for row in range(self.height):
            print("|", end=" ")
            for col in range(self.width):
                print(symbols[display_house[row, col]], end=" ")
            print("|")
        print("=" * (self.width * 2 + 3))


class SARSAAgent:
    """SARSA learning agent for the vacuum cleaner"""
    def __init__(self, env, learning_rate=0.1, discount_factor=0.95, 
                 epsilon=0.2, epsilon_decay=0.995, min_epsilon=0.05):
        self.env = env
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.min_epsilon = min_epsilon
        
        # Initialize Q-table as a dictionary (for sparse state representation)
        self.q_table = defaultdict(lambda: np.zeros(6))  # 6 actions
        
        # Action space: 0=up, 1=right, 2=down, 3=left, 4=clean, 5=charge
        
    def discretize_state(self, state):
        """Convert continuous state to discrete representation for Q-table"""
        row, col, energy, is_dirty = state
        
        # Discretize energy levels: low, medium, high
        if energy < 30:
            energy_level = 0
        elif energy < 70:
            energy_level = 1
        else:
            energy_level = 2
            
        # Create discrete state representation
        discrete_state = (row, col, energy_level, is_dirty)
        return discrete_state
    
    def choose_action(self, state, training=True):
        """Epsilon-greedy action selection"""
        discrete_state = self.discretize_state(state)
        
        # Exploration vs exploitation
        if training and random.random() < self.epsilon:
            # Explore: choose random action
            return random.randint(0, 5)
        else:
            # Exploit: choose best action from Q-table
            return np.argmax(self.q_table[discrete_state])
    
    def update_q_value(self, state, action, reward, next_state, next_action):
        """Update Q-value using SARSA formula"""
        discrete_state = self.discretize_state(state)
        discrete_next_state = self.discretize_state(next_state)
        
        # Get current Q-value
        current_q = self.q_table[discrete_state][action]
        
        # Get next Q-value (from next action we'll take)
        next_q = self.q_table[discrete_next_state][next_action]
        
        # SARSA update formula:
        # Q(s,a) = Q(s,a) + Î± * [r + Î³ * Q(s',a') - Q(s,a)]
        td_target = reward + self.discount_factor * next_q
        td_error = td_target - current_q
        
        # Update Q-value
        self.q_table[discrete_state][action] += self.learning_rate * td_error
        
        return td_error
    
    def decay_epsilon(self):
        """Gradually reduce exploration rate"""
        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)
    
    def train(self, episodes=1000, render_freq=100):
        """Train the agent using SARSA"""
        print("Starting SARSA training for Robot Vacuum Cleaner...")
        
        episode_rewards = []
        episode_cleaned = []
        episode_energy = []
        
        for episode in range(episodes):
            state = self.env.reset()
            action = self.choose_action(state)
            
            done = False
            total_reward = 0
            steps = 0
            
            while not done:
                # Take action
                next_state, reward, done = self.env.step(action)
                
                # Choose next action (important for SARSA)
                next_action = self.choose_action(next_state)
                
                # Update Q-value using SARSA
                self.update_q_value(state, action, reward, next_state, next_action)
                
                # Move to next state-action pair
                state = next_state
                action = next_action
                
                total_reward += reward
                steps += 1
            
            # Decay epsilon after each episode
            self.decay_epsilon()
            
            # Record metrics
            episode_rewards.append(total_reward)
            episode_cleaned.append(self.env.cleaned_area)
            episode_energy.append(self.env.current_energy)
            
            # Print progress
            if (episode + 1) % render_freq == 0:
                avg_reward = np.mean(episode_rewards[-render_freq:])
                avg_cleaned = np.mean(episode_cleaned[-render_freq:])
                print(f"Episode {episode + 1}/{episodes}, "
                      f"Avg Reward: {avg_reward:.2f}, "
                      f"Avg Cleaned: {avg_cleaned:.1f}, "
                      f"Epsilon: {self.epsilon:.3f}")
                
                # Show a demonstration
                if episode == episodes - 1:
                    print("\nFinal demonstration:")
                    self.env.render()
        
        return episode_rewards, episode_cleaned, episode_energy
    
    def test_policy(self, episodes=5):
        """Test the learned policy without exploration"""
        print("\n" + "="*50)
        print("Testing learned policy...")
        print("="*50)
        
        for episode in range(episodes):
            state = self.env.reset()
            action = self.choose_action(state, training=False)
            
            done = False
            total_reward = 0
            steps = 0
            
            print(f"\nTest Episode {episode + 1}:")
            self.env.render()
            time.sleep(0.5)
            
            while not done and steps < 50:
                # Take action
                next_state, reward, done = self.env.step(action)
                
                # Choose next action (greedy)
                next_action = self.choose_action(next_state, training=False)
                
                state = next_state
                action = next_action
                total_reward += reward
                steps += 1
                
                if steps % 10 == 0:
                    self.env.render()
                    time.sleep(0.3)
            
            print(f"Test Episode {episode + 1} completed:")
            print(f"  Steps: {steps}, Reward: {total_reward:.1f}")
            print(f"  Area cleaned: {self.env.cleaned_area}")
            print(f"  Remaining energy: {self.env.current_energy}")


def plot_results(rewards, cleaned_areas, energies):
    """Plot training results"""
    fig, axes = plt.subplots(3, 1, figsize=(10, 12))
    
    # Plot rewards
    axes[0].plot(rewards, alpha=0.6, linewidth=0.8)
    axes[0].set_title('Episode Rewards')
    axes[0].set_xlabel('Episode')
    axes[0].set_ylabel('Total Reward')
    axes[0].grid(True, alpha=0.3)
    
    # Plot moving average of rewards
    window_size = 50
    moving_avg = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')
    axes[0].plot(range(window_size-1, len(rewards)), moving_avg, 'r-', linewidth=2, label=f'Moving Avg ({window_size} eps)')
    axes[0].legend()
    
    # Plot cleaned areas
    axes[1].plot(cleaned_areas, alpha=0.6, linewidth=0.8, color='green')
    axes[1].set_title('Area Cleaned per Episode')
    axes[1].set_xlabel('Episode')
    axes[1].set_ylabel('Number of Dirty Cells Cleaned')
    axes[1].grid(True, alpha=0.3)
    
    # Plot moving average of cleaned areas
    moving_avg_clean = np.convolve(cleaned_areas, np.ones(window_size)/window_size, mode='valid')
    axes[1].plot(range(window_size-1, len(cleaned_areas)), moving_avg_clean, 'b-', linewidth=2, label=f'Moving Avg ({window_size} eps)')
    axes[1].legend()
    
    # Plot remaining energy
    axes[2].plot(energies, alpha=0.6, linewidth=0.8, color='orange')
    axes[2].set_title('Remaining Energy per Episode')
    axes[2].set_xlabel('Episode')
    axes[2].set_ylabel('Energy Level')
    axes[2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()


def print_policy_insights(agent, env):
    """Print insights about the learned policy"""
    print("\n" + "="*50)
    print("Learned Policy Insights")
    print("="*50)
    
    # Check policy at different states
    test_states = [
        (0, 0, 100, 0),  # At charger with full energy
        (4, 4, 50, 1),   # Middle of room, medium energy, dirty spot
        (4, 4, 20, 0),   # Middle of room, low energy, clean spot
    ]
    
    action_names = ['Up', 'Right', 'Down', 'Left', 'Clean', 'Charge']
    
    for state in test_states:
        discrete_state = agent.discretize_state(state)
        q_values = agent.q_table[discrete_state]
        best_action = np.argmax(q_values)
        
        print(f"\nState: Position({state[0]},{state[1]}), "
              f"Energy: {state[2]}, Dirty: {state[3]}")
        print(f"Best action: {action_names[best_action]}")
        print(f"Q-values: {[f'{q:.2f}' for q in q_values]}")


def main():
    """Main function to run the SARSA vacuum cleaner"""
    # Create environment
    env = HouseEnvironment(width=8, height=8)
    
    # Create SARSA agent
    agent = SARSAAgent(
        env=env,
        learning_rate=0.1,
        discount_factor=0.95,
        epsilon=0.3,
        epsilon_decay=0.998,
        min_epsilon=0.05
    )
    
    # Display initial house state
    print("Initial House State:")
    print("ðŸ¤– = Vacuum, âš¡ = Charger, * = Dirty, â–ˆ = Furniture, Â· = Clean")
    env.render()
    
    # Train the agent
    print("\n" + "="*50)
    print("Starting Training Phase")
    print("="*50)
    input("\nPress Enter to start training...")
    
    rewards, cleaned_areas, energies = agent.train(episodes=800, render_freq=100)
    
    # Test the learned policy
    agent.test_policy(episodes=3)
    
    # Print policy insights
    print_policy_insights(agent, env)
    
    # Plot results
    print("\nGenerating training plots...")
    plot_results(rewards, cleaned_areas, energies)
    
    # Summary
    print("\n" + "="*50)
    print("Training Summary")
    print("="*50)
    print(f"Total episodes trained: {len(rewards)}")
    print(f"Average final reward: {np.mean(rewards[-100:]):.2f}")
    print(f"Average area cleaned: {np.mean(cleaned_areas[-100:]):.1f}")
    print(f"Final exploration rate (epsilon): {agent.epsilon:.3f}")
    
    # Show final demonstration
    input("\nPress Enter for final demonstration...")
    state = env.reset()
    done = False
    steps = 0
    
    while not done and steps < 30:
        action = agent.choose_action(state, training=False)
        next_state, reward, done = env.step(action)
        state = next_state
        steps += 1
        
        env.render()
        print(f"Action: {['Up', 'Right', 'Down', 'Left', 'Clean', 'Charge'][action]}, "
              f"Reward: {reward:.1f}")
        time.sleep(0.5)
    
    print("\nSARSA training completed successfully!")


if __name__ == "__main__":
    main()
