import numpy as np
import random
import time

class MazeEnv:
    """Simple maze environment"""
    def __init__(self, width=5, height=5):
        self.width = width
        self.height = height
        self.reset()
        
        # Define maze elements
        # 0 = empty, 1 = wall, 2 = trap, 3 = goal, 4 = player
        self.maze = np.zeros((height, width), dtype=int)
        
        # Add some traps
        traps = [(1, 1), (3, 0), (2, 3), (4, 2)]
        for trap in traps:
            if 0 <= trap[0] < height and 0 <= trap[1] < width:
                self.maze[trap] = 2
                
        # Set goal position
        self.goal_pos = (height-1, width-1)
        self.maze[self.goal_pos] = 3
        
        # Set start position
        self.start_pos = (0, 0)
        
    def reset(self):
        """Reset the agent to start position"""
        self.player_pos = (0, 0)
        return self.player_pos
    
    def step(self, action):
        """
        Take an action in the environment
        Actions: 0=up, 1=right, 2=down, 3=left
        """
        row, col = self.player_pos
        
        # Move based on action
        if action == 0:  # Up
            row = max(row - 1, 0)
        elif action == 1:  # Right
            col = min(col + 1, self.width - 1)
        elif action == 2:  # Down
            row = min(row + 1, self.height - 1)
        elif action == 3:  # Left
            col = max(col - 1, 0)
            
        new_pos = (row, col)
        
        # Check what's at the new position
        cell_value = self.maze[new_pos]
        
        # Calculate reward
        if cell_value == 2:  # Trap
            reward = -10
            done = True
        elif cell_value == 3:  # Goal
            reward = 10
            done = True
        else:  # Empty cell
            reward = -0.1  # Small penalty for each step to encourage efficiency
            done = False
            
        # Update player position if not a trap (if trap, reset to start)
        if cell_value != 2:
            self.player_pos = new_pos
        else:
            self.player_pos = self.start_pos
            
        return new_pos, reward, done
    
    def render(self):
        """Display the current state of the maze"""
        display_maze = self.maze.copy()
        if self.player_pos != self.goal_pos:  # Don't overwrite goal with player
            display_maze[self.player_pos] = 4
            
        symbols = {
            0: '.',  # Empty
            1: '#',  # Wall
            2: 'T',  # Trap
            3: 'G',  # Goal
            4: 'P'   # Player
        }
        
        print("\n" + "=" * (self.width * 2 + 3))
        for row in range(self.height):
            print("|", end=" ")
            for col in range(self.width):
                print(symbols[display_maze[row, col]], end=" ")
            print("|")
        print("=" * (self.width * 2 + 3))


class TD0Agent:
    """TD(0) learning agent"""
    def __init__(self, env, learning_rate=0.1, discount_factor=0.9, epsilon=0.1):
        self.env = env
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon  # For epsilon-greedy policy
        
        # Initialize value function for each state (position)
        self.values = np.zeros((env.height, env.width))
        
        # Initialize Q-values for state-action pairs
        # Shape: (height, width, num_actions)
        self.q_values = np.zeros((env.height, env.width, 4))
        
    def choose_action(self, state, epsilon=None):
        """Epsilon-greedy action selection"""
        if epsilon is None:
            epsilon = self.epsilon
            
        row, col = state
        
        # Random action with probability epsilon
        if random.random() < epsilon:
            return random.randint(0, 3)
        # Otherwise choose best action based on Q-values
        else:
            return np.argmax(self.q_values[row, col])
    
    def update_value_function(self, state, reward, next_state, done):
        """Update the value function using TD(0)"""
        row, col = state
        next_row, next_col = next_state
        
        # TD(0) update: V(s) = V(s) + α * [r + γ * V(s') - V(s)]
        if done:
            td_target = reward
        else:
            td_target = reward + self.discount_factor * self.values[next_row, next_col]
            
        td_error = td_target - self.values[row, col]
        self.values[row, col] += self.learning_rate * td_error
        
        return td_error
    
    def update_q_values(self, state, action, reward, next_state, done):
        """Update Q-values using TD(0)"""
        row, col = state
        next_row, next_col = next_state
        
        # Get the maximum Q-value for the next state
        if done:
            max_next_q = 0
        else:
            max_next_q = np.max(self.q_values[next_row, next_col])
        
        # TD(0) update for Q-values
        current_q = self.q_values[row, col, action]
        td_target = reward + self.discount_factor * max_next_q
        td_error = td_target - current_q
        
        self.q_values[row, col, action] += self.learning_rate * td_error
        
        return td_error
    
    def train(self, episodes=500, render_freq=50):
        """Train the agent using TD(0)"""
        print("Starting TD(0) training...")
        
        episode_rewards = []
        
        for episode in range(episodes):
            state = self.env.reset()
            done = False
            total_reward = 0
            steps = 0
            
            while not done and steps < 100:  # Limit steps per episode
                # Choose action
                action = self.choose_action(state)
                
                # Take action
                next_state, reward, done = self.env.step(action)
                
                # Update value function and Q-values
                self.update_value_function(state, reward, next_state, done)
                self.update_q_values(state, action, reward, next_state, done)
                
                # Move to next state
                state = next_state
                total_reward += reward
                steps += 1
            
            episode_rewards.append(total_reward)
            
            # Print progress
            if (episode + 1) % render_freq == 0:
                avg_reward = np.mean(episode_rewards[-render_freq:])
                print(f"Episode {episode + 1}/{episodes}, "
                      f"Average Reward (last {render_freq}): {avg_reward:.2f}")
                
                # Show the learned values
                if episode == episodes - 1:
                    print("\nLearned State Values:")
                    print(np.round(self.values, 2))
        
        return episode_rewards
    
    def get_optimal_path(self, start_pos=(0, 0)):
        """Find the optimal path using the learned policy"""
        path = [start_pos]
        state = start_pos
        visited = set([start_pos])
        done = False
        steps = 0
        
        while not done and steps < 50:
            row, col = state
            
            # Choose best action (greedy)
            action = np.argmax(self.q_values[row, col])
            
            # Simulate the action
            if action == 0:  # Up
                next_state = (max(row - 1, 0), col)
            elif action == 1:  # Right
                next_state = (row, min(col + 1, self.env.width - 1))
            elif action == 2:  # Down
                next_state = (min(row + 1, self.env.height - 1), col)
            elif action == 3:  # Left
                next_state = (row, max(col - 1, 0))
            
            # Check if we reached the goal or a trap
            if next_state == self.env.goal_pos:
                path.append(next_state)
                print("Reached the goal!")
                done = True
            elif self.env.maze[next_state] == 2:  # Trap
                path.append(next_state)
                print("Hit a trap!")
                done = True
            else:
                # Avoid loops
                if next_state in visited:
                    print("Loop detected!")
                    break
                path.append(next_state)
                visited.add(next_state)
                state = next_state
            
            steps += 1
        
        return path


def main():
    """Main function to run the TD(0) maze solver"""
    # Create environment
    env = MazeEnv(width=5, height=5)
    
    # Create agent
    agent = TD0Agent(env, learning_rate=0.1, discount_factor=0.9, epsilon=0.1)
    
    # Display initial maze
    print("Initial Maze:")
    print("P = Player, G = Goal, T = Trap")
    env.render()
    
    # Train the agent
    input("\nPress Enter to start training...")
    rewards = agent.train(episodes=500, render_freq=50)
    
    # Show the optimal path
    input("\nPress Enter to show the optimal path...")
    print("\nFinding optimal path from start to goal...")
    path = agent.get_optimal_path()
    
    print(f"\nOptimal Path ({len(path)} steps):")
    for i, pos in enumerate(path):
        print(f"Step {i}: {pos}")
    
    # Visualize the path on the maze
    print("\nPath visualization on the maze:")
    display_maze = env.maze.copy()
    for i, (row, col) in enumerate(path):
        if (row, col) != env.goal_pos:  # Don't overwrite goal
            display_maze[row, col] = 4 if i == 0 else 5  # 5 for path markers
    
    symbols = {
        0: '.',  # Empty
        1: '#',  # Wall
        2: 'T',  # Trap
        3: 'G',  # Goal
        4: 'P',  # Player (start)
        5: 'o'   # Path
    }
    
    print("\n" + "=" * (env.width * 2 + 3))
    for row in range(env.height):
        print("|", end=" ")
        for col in range(env.width):
            print(symbols[display_maze[row, col]], end=" ")
        print("|")
    print("=" * (env.width * 2 + 3))
    
    # Demonstrate the agent in action
    input("\nPress Enter to see the agent navigate the maze...")
    print("\nAgent navigating the maze:")
    env.reset()
    env.render()
    
    done = False
    steps = 0
    while not done and steps < 20:
        state = env.player_pos
        action = agent.choose_action(state, epsilon=0)  # Greedy policy
        next_state, reward, done = env.step(action)
        
        time.sleep(0.5)
        env.render()
        print(f"Action: {['Up', 'Right', 'Down', 'Left'][action]}, Reward: {reward}")
        
        steps += 1
    
    print(f"\nTraining completed! Final reward: {reward}")


if __name__ == "__main__":
    main()
