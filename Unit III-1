import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque
import matplotlib.pyplot as plt

class CleaningRobotEnv:
    """Environment for the cleaning robot"""
    def __init__(self, grid_size=5):
        self.grid_size = grid_size
        self.reset()
        
    def create_grid(self):
        """Create a new grid with dirt and obstacles"""
        grid = np.zeros((self.grid_size, self.grid_size), dtype=int)
        
        # Add dirt (80% of cells, except start position)
        dirt_cells = int(0.8 * (self.grid_size * self.grid_size - 1))
        positions = [(i, j) for i in range(self.grid_size) for j in range(self.grid_size) if (i, j) != (0, 0)]
        dirt_positions = random.sample(positions, dirt_cells)
        
        for pos in dirt_positions:
            grid[pos] = 1  # Dirt
        
        # Add obstacles (3-4 obstacles)
        num_obstacles = random.randint(3, 4)
        obstacle_positions = random.sample([p for p in positions if p not in dirt_positions], num_obstacles)
        
        for pos in obstacle_positions:
            grid[pos] = -1  # Obstacle
            
        return grid
    
    def reset(self):
        """Reset the environment"""
        self.grid = self.create_grid()
        self.robot_pos = (0, 0)  # Start at top-left corner
        self.total_reward = 0
        self.steps = 0
        self.dirt_collected = 0
        self.total_dirt = np.sum(self.grid == 1)
        self.done = False
        return self.get_state()
    
    def get_state(self):
        """Get the current state representation"""
        row, col = self.robot_pos
        
        # Create a simple state representation
        # 1. Current position (flattened)
        # 2. Whether current cell has dirt
        # 3. Whether adjacent cells are obstacles or boundaries
        state = []
        
        # Add current position (one-hot encoded)
        pos_vector = np.zeros(self.grid_size * self.grid_size)
        pos_idx = row * self.grid_size + col
        pos_vector[pos_idx] = 1
        state.extend(pos_vector)
        
        # Add dirt at current position
        state.append(1 if self.grid[row, col] == 1 else 0)
        
        # Add information about adjacent cells (up, right, down, left)
        for dr, dc in [(-1, 0), (0, 1), (1, 0), (0, -1)]:
            new_r, new_c = row + dr, col + dc
            if 0 <= new_r < self.grid_size and 0 <= new_c < self.grid_size:
                if self.grid[new_r, new_c] == -1:  # Obstacle
                    state.append(-1)
                elif self.grid[new_r, new_c] == 1:  # Dirt
                    state.append(1)
                else:  # Clean
                    state.append(0)
            else:  # Boundary
                state.append(-1)
        
        return np.array(state, dtype=np.float32)
    
    def step(self, action):
        """
        Take an action in the environment
        Actions: 0=up, 1=right, 2=down, 3=left, 4=clean
        """
        if self.done:
            return self.get_state(), 0, True
        
        row, col = self.robot_pos
        reward = 0
        self.steps += 1
        
        if action == 0:  # Up
            new_row, new_col = max(row - 1, 0), col
        elif action == 1:  # Right
            new_row, new_col = row, min(col + 1, self.grid_size - 1)
        elif action == 2:  # Down
            new_row, new_col = min(row + 1, self.grid_size - 1), col
        elif action == 3:  # Left
            new_row, new_col = row, max(col - 1, 0)
        elif action == 4:  # Clean
            new_row, new_col = row, col
            if self.grid[row, col] == 1:  # Cell has dirt
                self.grid[row, col] = 0  # Clean it
                reward = 1
                self.dirt_collected += 1
            else:
                reward = -0.1  # Small penalty for cleaning clean cell
        else:
            new_row, new_col = row, col
        
        # Check if new position is obstacle
        if self.grid[new_row, new_col] == -1:
            reward = -1
            new_row, new_col = row, col  # Don't move into obstacle
        else:
            # Small penalty for movement to encourage efficiency
            if (new_row, new_col) != (row, col):
                reward -= 0.01
        
        # Update robot position
        self.robot_pos = (new_row, new_col)
        
        # Check if all dirt is cleaned or max steps reached
        if self.dirt_collected >= self.total_dirt:
            reward += 5  # Bonus for cleaning everything
            self.done = True
        elif self.steps >= 100:  # Max steps per episode
            self.done = True
        
        self.total_reward += reward
        return self.get_state(), reward, self.done
    
    def render(self):
        """Display the current state of the grid"""
        print(f"\nStep: {self.steps}, Reward: {self.total_reward:.2f}, Dirt: {self.dirt_collected}/{self.total_dirt}")
        print("+" + "-" * (self.grid_size * 2 + 1) + "+")
        
        for i in range(self.grid_size):
            print("|", end=" ")
            for j in range(self.grid_size):
                if (i, j) == self.robot_pos:
                    print("R", end=" ")
                elif self.grid[i, j] == 1:
                    print("·", end=" ")  # Dirt
                elif self.grid[i, j] == -1:
                    print("█", end=" ")  # Obstacle
                else:
                    print(" ", end=" ")  # Clean
            print("|")
        
        print("+" + "-" * (self.grid_size * 2 + 1) + "+")


class PolicyNetwork(nn.Module):
    """Neural network for the policy"""
    def __init__(self, state_size, action_size, hidden_size=64):
        super(PolicyNetwork, self).__init__()
        
        # Neural network layers
        self.fc1 = nn.Linear(state_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, action_size)
        
    def forward(self, state):
        """Forward pass through the network"""
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return F.softmax(x, dim=-1)  # Probability distribution over actions


class REINFORCEAgent:
    """REINFORCE algorithm implementation"""
    def __init__(self, env, learning_rate=0.001, gamma=0.99):
        self.env = env
        self.gamma = gamma  # Discount factor
        
        # Get state and action sizes
        state_size = len(env.get_state())
        action_size = 5  # 4 movements + clean
        
        # Initialize policy network
        self.policy_net = PolicyNetwork(state_size, action_size)
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)
        
        # Storage for episode data
        self.log_probs = []
        self.rewards = []
        
    def select_action(self, state):
        """Select an action based on the current policy"""
        state_tensor = torch.from_numpy(state).float().unsqueeze(0)
        
        # Get action probabilities from policy network
        with torch.no_grad():
            action_probs = self.policy_net(state_tensor)
        
        # Create a categorical distribution based on the probabilities
        dist = torch.distributions.Categorical(action_probs)
        
        # Sample an action from the distribution
        action = dist.sample()
        
        # Store the log probability of the selected action
        self.log_probs.append(dist.log_prob(action))
        
        return action.item()
    
    def update_policy(self):
        """Update the policy using the REINFORCE algorithm"""
        # Calculate returns (discounted cumulative rewards)
        returns = []
        R = 0
        
        # Calculate returns from the end to the beginning
        for r in reversed(self.rewards):
            R = r + self.gamma * R
            returns.insert(0, R)
        
        # Normalize returns for stability
        returns = torch.tensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)
        
        # Calculate policy loss
        policy_loss = []
        for log_prob, R in zip(self.log_probs, returns):
            policy_loss.append(-log_prob * R)  # Negative because we want to maximize
        
        # Combine all losses and backpropagate
        self.optimizer.zero_grad()
        total_loss = torch.stack(policy_loss).sum()
        total_loss.backward()
        self.optimizer.step()
        
        # Clear the episode data
        self.log_probs = []
        self.rewards = []
        
        return total_loss.item()
    
    def train(self, episodes=1000, render_freq=100):
        """Train the agent using REINFORCE"""
        print("Training Cleaning Robot with REINFORCE algorithm...")
        print("="*50)
        
        episode_rewards = []
        episode_dirt = []
        episode_lengths = []
        
        for episode in range(episodes):
            # Reset environment and clear episode data
            state = self.env.reset()
            self.log_probs = []
            self.rewards = []
            
            total_reward = 0
            steps = 0
            done = False
            
            # Run one episode
            while not done:
                # Select and take action
                action = self.select_action(state)
                next_state, reward, done = self.env.step(action)
                
                # Store reward
                self.rewards.append(reward)
                
                # Update state and counters
                state = next_state
                total_reward += reward
                steps += 1
            
            # Update policy after episode
            loss = self.update_policy()
            
            # Record metrics
            episode_rewards.append(total_reward)
            episode_dirt.append(self.env.dirt_collected)
            episode_lengths.append(steps)
            
            # Print progress
            if (episode + 1) % render_freq == 0:
                avg_reward = np.mean(episode_rewards[-render_freq:])
                avg_dirt = np.mean(episode_dirt[-render_freq:])
                avg_steps = np.mean(episode_lengths[-render_freq:])
                
                print(f"Episode {episode + 1}/{episodes}")
                print(f"  Avg Reward: {avg_reward:.2f}")
                print(f"  Avg Dirt Collected: {avg_dirt:.1f}/{self.env.total_dirt}")
                print(f"  Avg Steps: {avg_steps:.1f}")
                print(f"  Policy Loss: {loss:.4f}")
                print("-" * 30)
                
                # Show current policy in action
                if episode == episodes - 1:
                    print("\nFinal demonstration:")
                    self.env.render()
        
        return episode_rewards, episode_dirt, episode_lengths
    
    def test(self, num_episodes=10):
        """Test the trained agent"""
        print("\n" + "="*50)
        print("Testing Trained Agent")
        print("="*50)
        
        test_rewards = []
        test_dirt = []
        test_steps = []
        
        for episode in range(num_episodes):
            state = self.env.reset()
            total_reward = 0
            steps = 0
            done = False
            
            print(f"\nTest Episode {episode + 1}:")
            self.env.render()
            
            while not done:
                # Select action (without storing log probs)
                state_tensor = torch.from_numpy(state).float().unsqueeze(0)
                with torch.no_grad():
                    action_probs = self.policy_net(state_tensor)
                action = torch.argmax(action_probs).item()
                
                # Take action
                state, reward, done = self.env.step(action)
                total_reward += reward
                steps += 1
                
                # Render every 5 steps
                if steps % 5 == 0:
                    self.env.render()
            
            # Record results
            test_rewards.append(total_reward)
            test_dirt.append(self.env.dirt_collected)
            test_steps.append(steps)
            
            print(f"  Final Reward: {total_reward:.2f}")
            print(f"  Dirt Collected: {self.env.dirt_collected}/{self.env.total_dirt}")
            print(f"  Steps: {steps}")
        
        # Print summary
        print("\n" + "="*50)
        print("Test Results Summary")
        print("="*50)
        print(f"Average Reward: {np.mean(test_rewards):.2f} ± {np.std(test_rewards):.2f}")
        print(f"Average Dirt Collected: {np.mean(test_dirt):.1f}/{self.env.total_dirt}")
        print(f"Average Steps: {np.mean(test_steps):.1f}")
        print(f"Success Rate: {sum([d == self.env.total_dirt for d in test_dirt])/num_episodes*100:.1f}%")
        
        return test_rewards, test_dirt, test_steps


def plot_training_results(rewards, dirt_collected, steps):
    """Plot training results"""
    try:
        fig, axes = plt.subplots(3, 1, figsize=(10, 10))
        
        # Plot rewards
        axes[0].plot(rewards, alpha=0.6, linewidth=0.8)
        axes[0].set_title('Episode Rewards')
        axes[0].set_xlabel('Episode')
        axes[0].set_ylabel('Total Reward')
        axes[0].grid(True, alpha=0.3)
        
        # Add moving average
        window = 50
        if len(rewards) > window:
            moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')
            axes[0].plot(range(window-1, len(rewards)), moving_avg, 'r-', linewidth=2, 
                        label=f'Moving Avg ({window} eps)')
            axes[0].legend()
        
        # Plot dirt collected
        axes[1].plot(dirt_collected, alpha=0.6, linewidth=0.8, color='green')
        axes[1].set_title('Dirt Collected per Episode')
        axes[1].set_xlabel('Episode')
        axes[1].set_ylabel('Dirt Collected')
        axes[1].axhline(y=20, color='r', linestyle='--', alpha=0.5, label='Max Possible')
        axes[1].grid(True, alpha=0.3)
        axes[1].legend()
        
        # Plot episode lengths
        axes[2].plot(steps, alpha=0.6, linewidth=0.8, color='orange')
        axes[2].set_title('Episode Lengths (Steps)')
        axes[2].set_xlabel('Episode')
        axes[2].set_ylabel('Steps')
        axes[2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
    except ImportError:
        print("\nMatplotlib not installed. Skipping plots.")


def print_policy_insights(agent, env):
    """Print insights about the learned policy"""
    print("\n" + "="*50)
    print("Policy Insights")
    print("="*50)
    
    action_names = ['Up', 'Right', 'Down', 'Left', 'Clean']
    
    # Test some common states
    test_states = []
    
    # State 1: Starting position
    env.reset()
    state1 = env.get_state()
    test_states.append(("Start position", state1))
    
    # State 2: Middle with dirt
    env.grid = np.zeros((5, 5))
    env.grid[2, 2] = 1  # Dirt in middle
    env.robot_pos = (2, 2)
    state2 = env.get_state()
    test_states.append(("On dirt", state2))
    
    # State 3: Near obstacle
    env.grid = np.zeros((5, 5))
    env.grid[1, 2] = -1  # Obstacle below
    env.robot_pos = (0, 2)
    state3 = env.get_state()
    test_states.append(("Above obstacle", state3))
    
    # Evaluate each test state
    for name, state in test_states:
        state_tensor = torch.from_numpy(state).float().unsqueeze(0)
        with torch.no_grad():
            action_probs = agent.policy_net(state_tensor)
        
        probs = action_probs.squeeze().numpy()
        best_action = np.argmax(probs)
        
        print(f"\n{name}:")
        for i, (action_name, prob) in enumerate(zip(action_names, probs)):
            marker = " ← BEST" if i == best_action else ""
            print(f"  {action_name}: {prob:.3f}{marker}")


def main():
    """Main function to run REINFORCE for cleaning robot"""
    print("="*60)
    print("AUTONOMOUS CLEANING ROBOT - REINFORCE ALGORITHM")
    print("="*60)
    print("\nGrid Elements:")
    print("R = Robot")
    print("· = Dirt (+1 reward when cleaned)")
    print("█ = Obstacle (-1 penalty for hitting)")
    print("  = Clean cell")
    
    # Create environment
    env = CleaningRobotEnv(grid_size=5)
    
    # Create REINFORCE agent
    agent = REINFORCEAgent(env, learning_rate=0.001, gamma=0.99)
    
    # Show initial environment
    print("\nInitial Environment:")
    env.render()
    
    # Train the agent
    print("\n" + "="*60)
    print("TRAINING PHASE")
    print("="*60)
    
    input("\nPress Enter to start training...")
    
    rewards, dirt_collected, steps = agent.train(episodes=800, render_freq=100)
    
    # Plot training results
    plot_training_results(rewards, dirt_collected, steps)
    
    # Print policy insights
    print_policy_insights(agent, env)
    
    # Test the trained agent
    test_rewards, test_dirt, test_steps = agent.test(num_episodes=5)
    
    # Final demonstration
    print("\n" + "="*60)
    print("FINAL DEMONSTRATION")
    print("="*60)
    
    input("\nPress Enter for final demonstration...")
    
    # Run one more episode with rendering
    state = env.reset()
    total_reward = 0
    done = False
    
    print("\nStarting final demonstration...")
    env.render()
    
    while not done:
        state_tensor = torch.from_numpy(state).float().unsqueeze(0)
        with torch.no_grad():
            action_probs = agent.policy_net(state_tensor)
        action = torch.argmax(action_probs).item()
        
        action_names = ['↑', '→', '↓', '←', 'C']
        print(f"\nAction: {action_names[action]} ({['Up', 'Right', 'Down', 'Left', 'Clean'][action]})")
        
        state, reward, done = env.step(action)
        total_reward += reward
        
        env.render()
        
        if not done:
            input("Press Enter for next step...")
    
    print("\n" + "="*60)
    print("TRAINING COMPLETE!")
    print("="*60)
    print(f"\nThe cleaning robot has learned to efficiently clean the grid!")
    print(f"Final episode: Collected {env.dirt_collected}/{env.total_dirt} dirt")
    print(f"Total reward: {total_reward:.2f}")
    print(f"Steps taken: {env.steps}")


if __name__ == "__main__":
    # Set random seeds for reproducibility
    random.seed(42)
    np.random.seed(42)
    torch.manual_seed(42)
    
    main()
